import os
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import pipeline
import PyPDF2

# 1ï¸âƒ£ HÃ m Ä‘á»c toÃ n bá»™ PDF trong thÆ° má»¥c
def read_all_pdfs(folder_path):
    all_text = ""
    for filename in os.listdir(folder_path):
        if filename.lower().endswith(".pdf"):
            file_path = os.path.join(folder_path, filename)
            print(f"ğŸ“˜ Äang Ä‘á»c: {filename}")
            with open(file_path, "rb") as f:
                reader = PyPDF2.PdfReader(f)
                for page in reader.pages:
                    page_text = page.extract_text()
                    if page_text:
                        all_text += page_text + "\n"
    return all_text

# Äá»c táº¥t cáº£ PDF trong thÆ° má»¥c
folder = "data"  # ğŸ‘‰ Ä‘á»•i thÃ nh Ä‘Æ°á»ng dáº«n thÆ° má»¥c PDF cá»§a báº¡n
text = read_all_pdfs(folder)

# 2ï¸âƒ£ Chia vÄƒn báº£n thÃ nh cÃ¡c Ä‘oáº¡n (chunk)
# chia theo Ä‘oáº¡n trá»‘ng hoáº·c má»—i 500 tá»« Ä‘á»ƒ giá»¯ ngá»¯ cáº£nh
def split_into_chunks(text, max_words=500):
    words = text.split()
    chunks = []
    for i in range(0, len(words), max_words):
        chunk = " ".join(words[i:i+max_words])
        chunks.append(chunk)
    return chunks

chunks = split_into_chunks(text)

# 3ï¸âƒ£ Táº¡o embedding vector cho tá»«ng chunk
print("ğŸ” Äang táº¡o embedding...")
embed_model = SentenceTransformer('paraphrase-MiniLM-L3-v2')
chunk_embeddings = embed_model.encode(chunks, show_progress_bar=True)

# 4ï¸âƒ£ HÃ m tÃ­nh cosine similarity
def cosine_sim(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

# 5ï¸âƒ£ HÃ m tÃ¬m top chunk liÃªn quan
def get_top_chunks(question, top_k=2):
    q_emb = embed_model.encode([question])[0]
    sims = [cosine_sim(q_emb, c_emb) for c_emb in chunk_embeddings]
    top_idx = np.argsort(sims)[::-1][:top_k]
    return [chunks[i] for i in top_idx]

# 6ï¸âƒ£ CÃ¢u há»i vÃ­ dá»¥
question = "What is the function of an inverter in a solar energy system?"

top_chunks = get_top_chunks(question, top_k=2)
context = "\n".join(top_chunks)

# 7ï¸âƒ£ Táº¡o prompt
prompt = f"""
DÆ°á»›i Ä‘Ã¢y lÃ  Ä‘oáº¡n tÃ i liá»‡u tham kháº£o:
{context}

HÃ£y tráº£ lá»i cÃ¢u há»i sau dá»±a vÃ o Ä‘oáº¡n trÃªn:
{question}
"""

# 8ï¸âƒ£ Tráº£ lá»i báº±ng mÃ´ hÃ¬nh Hugging Face
qa_pipeline = pipeline("text2text-generation", model="google/flan-t5-small")
answer = qa_pipeline(prompt, max_length=256)[0]['generated_text']

print("\nğŸ§  CÃ¢u há»i:", question)
print("\nğŸ“œ Ngá»¯ cáº£nh:\n", context[:1000], "...")
print("\nğŸ¤– AI tráº£ lá»i:\n", answer)
